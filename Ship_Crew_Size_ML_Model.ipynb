{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Model for Predicting a Ship's Crew Size \n",
    "We build a simple model using the cruise_ship_info.csv data set for predicting a ship's crew size. This project is organized as follows: (a) data proprocessing and variable selection; (b) basic regression model; (c) hyper-parameters tuning; and (d) techniques for dimensionality reduction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read dataset and display columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"cruise_ship_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Calculate basic statistics of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Age', 'Tonnage', 'passengers', 'length', 'cabins','passenger_density','crew']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df[cols], size=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations from part 2\n",
    "1) We observe that variables are on different scales, for sample the Age variable ranges from about 16 years to 48 years, while the Tonnage variable ranges from 2 to 220, see probability density plots below. It is therefore important that when a regression model is built using these variables, variables be brought to same scale either by standardizing or normalizing the data.\n",
    "\n",
    "2) We also observe that the target variable 'crew' correlates well with 4 predictor variables, namely, 'Tonnage', 'passengers', 'length', and 'cabins'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['Age'],bins=20)\n",
    "plt.title('probability distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['Tonnage'],bins=20)\n",
    "plt.title('probability distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Variable selection for predicting \"crew\" size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 (a) Calculation of covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Age', 'Tonnage', 'passengers', 'length', 'cabins','passenger_density','crew']\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "stdsc = StandardScaler()\n",
    "X_std = stdsc.fit_transform(df[cols].iloc[:,range(0,7)].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_mat =np.cov(X_std.T)\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.set(font_scale=1.5)\n",
    "hm = sns.heatmap(cov_mat,\n",
    "                 cbar=True,\n",
    "                 annot=True,\n",
    "                 square=True,\n",
    "                 fmt='.2f',\n",
    "                 annot_kws={'size': 12},\n",
    "                 yticklabels=cols,\n",
    "                 xticklabels=cols)\n",
    "plt.title('Covariance matrix showing correlation coefficients')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 (b) Selecting important variables (columns)\n",
    "From the covariance matrix plot above, we see that the \"crew\" variable correlates strongly with 4 predictor variables: \"Tonnage\", \"passengers\", \"length, and \"cabins\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_selected = ['Tonnage', 'passengers', 'length', 'cabins','crew']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[cols_selected].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[cols_selected].iloc[:,0:4].values    # features matrix \n",
    "y = df[cols_selected]['crew'].values        # target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. One-hot encoding for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(categorical_features=[0 ,1])\n",
    "pd.get_dummies(df[['Ship_name', 'Cruise_line','Age', 'Tonnage', 'passengers', 'length', 'cabins','passenger_density','crew']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.get_dummies(df[['Ship_name', 'Cruise_line','Age', 'Tonnage', 'passengers', 'length', 'cabins','passenger_density','crew']])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df2['Ship_name_Adventure'],df2['crew'])\n",
    "plt.xlabel('Ship_name_Adventure')\n",
    "plt.ylabel('crew')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations from part 4\n",
    "\n",
    "In order to build a simplified regression model, we shall focus only on ordinal features. The categorical features \"Ship_name\"\tand \"Cruise_line\" will not be used. A simple model built using only the 4 ordinal features \"Tonnage\", \"passengers\", \"length, and \"cabins\" will be simple to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Data partitioning into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df[cols_selected].iloc[:,0:4].values     \n",
    "y = df[cols_selected]['crew']                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Building a muilt-regression model\n",
    "\n",
    "Our machine learning regression model for predicting a ship's \"crew\" size can be expressed as:\n",
    "\n",
    "$$ \\hat{y}_{i} = w_0 + \\sum_{j=1}^{4} X_{ij} w_j $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "slr = LinearRegression()\n",
    "\n",
    "slr.fit(X_train, y_train)\n",
    "y_train_pred = slr.predict(X_train)\n",
    "y_test_pred = slr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_train_pred,  y_train_pred - y_train,\n",
    "            c='steelblue', marker='o', edgecolor='white',\n",
    "            label='Training data')\n",
    "plt.scatter(y_test_pred,  y_test_pred - y_test,\n",
    "            c='limegreen', marker='s', edgecolor='white',\n",
    "            label='Test data')\n",
    "plt.xlabel('Predicted values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.legend(loc='upper left')\n",
    "plt.hlines(y=0, xmin=-10, xmax=50, color='black', lw=2)\n",
    "plt.xlim([-10, 50])\n",
    "plt.tight_layout()\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print('MSE train: %.3f, test: %.3f' % (\n",
    "        mean_squared_error(y_train, y_train_pred),\n",
    "        mean_squared_error(y_test, y_test_pred)))\n",
    "print('R^2 train: %.3f, test: %.3f' % (\n",
    "        r2_score(y_train, y_train_pred),\n",
    "        r2_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slr.fit(X_train, y_train).intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slr.fit(X_train, y_train).coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Feature Standardization, Cross Validation, and Hyper-parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df[cols_selected].iloc[:,0:4].values     \n",
    "y = df[cols_selected]['crew']  \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_y = StandardScaler()\n",
    "sc_x = StandardScaler()\n",
    "y_std = sc_y.fit_transform(y_train[:, np.newaxis]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = []\n",
    "test_score = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.4, random_state=i)\n",
    "    y_train_std = sc_y.fit_transform(y_train[:, np.newaxis]).flatten()\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    pipe_lr = Pipeline([('scl', StandardScaler()),('pca', PCA(n_components=4)),('slr', LinearRegression())])\n",
    "    pipe_lr.fit(X_train, y_train_std)\n",
    "    y_train_pred_std=pipe_lr.predict(X_train)\n",
    "    y_test_pred_std=pipe_lr.predict(X_test)\n",
    "    y_train_pred=sc_y.inverse_transform(y_train_pred_std)\n",
    "    y_test_pred=sc_y.inverse_transform(y_test_pred_std)\n",
    "    train_score = np.append(train_score, r2_score(y_train, y_train_pred))\n",
    "    test_score = np.append(test_score, r2_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('R2 train: %.3f +/- %.3f' % (np.mean(train_score),np.std(train_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('R2 test: %.3f +/- %.3f' % (np.mean(test_score),np.std(test_score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Techniques of Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 (a) Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = []\n",
    "test_score = []\n",
    "cum_variance = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,5):\n",
    "    X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.4, random_state=0)\n",
    "    y_train_std = sc_y.fit_transform(y_train[:, np.newaxis]).flatten()\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    pipe_lr = Pipeline([('scl', StandardScaler()),('pca', PCA(n_components=i)),('slr', LinearRegression())])\n",
    "    pipe_lr.fit(X_train, y_train_std)\n",
    "    y_train_pred_std=pipe_lr.predict(X_train)\n",
    "    y_test_pred_std=pipe_lr.predict(X_test)\n",
    "    y_train_pred=sc_y.inverse_transform(y_train_pred_std)\n",
    "    y_test_pred=sc_y.inverse_transform(y_test_pred_std)\n",
    "    train_score = np.append(train_score, r2_score(y_train, y_train_pred))\n",
    "    test_score = np.append(test_score, r2_score(y_test, y_test_pred))\n",
    "    cum_variance = np.append(cum_variance, np.sum(pipe_lr.fit(X_train, y_train).named_steps['pca'].explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(cum_variance,train_score, label = 'train_score')\n",
    "plt.plot(cum_variance, train_score)\n",
    "plt.scatter(cum_variance,test_score, label = 'test_score')\n",
    "plt.plot(cum_variance, test_score)\n",
    "plt.xlabel('cumulative variance')\n",
    "plt.ylabel('R2_score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations: PCA\n",
    "\n",
    "We observe that by increasing the number of principal components from 1 to 4, the train and test scores improve. This is because with less components, there is high bias error in the model, since model is overly simplified. As we increase the number of principal components, the bias error will reduce, but complexity in the model increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 (b) Regularized Regression: Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.4, random_state=0)\n",
    "y_train_std = sc_y.fit_transform(y_train[:, np.newaxis]).flatten()\n",
    "X_train_std = sc_x.fit_transform(X_train)\n",
    "X_test_std = sc_x.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.linspace(0.01,0.4,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso(alpha=0.7)\n",
    "\n",
    "r2_train=[]\n",
    "r2_test=[]\n",
    "norm = []\n",
    "for i in range(10):\n",
    "    lasso = Lasso(alpha=alpha[i])\n",
    "    lasso.fit(X_train_std,y_train_std)\n",
    "    y_train_std=lasso.predict(X_train_std)\n",
    "    y_test_std=lasso.predict(X_test_std)\n",
    "    r2_train=np.append(r2_train,r2_score(y_train,sc_y.inverse_transform(y_train_std)))\n",
    "    r2_test=np.append(r2_test,r2_score(y_test,sc_y.inverse_transform(y_test_std)))\n",
    "    norm= np.append(norm,np.linalg.norm(lasso.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(alpha,r2_train,label='r2_train')\n",
    "plt.plot(alpha,r2_train)\n",
    "plt.scatter(alpha,r2_test,label='r2_test')\n",
    "plt.plot(alpha,r2_test)\n",
    "plt.scatter(alpha,norm,label = 'norm')\n",
    "plt.plot(alpha,norm)\n",
    "plt.ylim(-0.1,1)\n",
    "plt.xlim(0,.43)\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('R2_score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Observations: Lasso\n",
    "\n",
    "We observe that as the regularization parameter $\\alpha$ increases, the norm of the regression coefficients become smaller and smaller. This means more regression coefficients are forced to zero, which intend increases bias error (over simplification). The best value to balance bias-variance tradeoff is when $\\alpha$ is kept low, say $\\alpha = 0.1$ or less."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
